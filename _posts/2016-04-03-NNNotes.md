---
title: Methods of regularization in neural networks
---

Regularization- A compromise between reducing the cost function and small weights to prevent overfitting. Regularized networks are more resistant to learning peculiarities of noise


**Modification of cost function:**

L1 regularization- tends to concentrate the weight of the network in a relatively small number of high-importance connections

L2 regularization- implement lambda between .5 and 5.0 depending on size of training data. Weights shrink by amount proportional to w

**Modification of NN:**

Dropout- randomly assign half of the hidden neurons with activation of 0, leaving input and output as it is, run mini-batch, then repeat process. When running full network, half all the weights. Especially userful in training deep and large networks where overfitting is acute. 

DropConnect- randomly cut weights from layer of neurons, across all neurons in that layer.

softmax- converts output activations to probablility distribution that adds up to 1. Useful for when result is one of several outcomes

**Modification of training data:**

artificially expanding training data- many different permutations (rotation, for example) of the training data to account for variations that may be found in the real world

